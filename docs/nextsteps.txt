Next steps


Ok so now we have some working processes

- We have the ability to take a source file and use “timecode_black_frames_relative.sh” which will make us a “BlankRush” that has burnt in TC and can we be used for our base of our mirrored source roundtrip files. 
- main.swift can handle taking the blank rush placing the graded segments into it and saving that via ProRes passthrough very quickly.


—— 

The top line goal:  Create a GUI Utility App that user can make a Project, import Graded Segments, as well point to the source editing files (OCF or ingested source files).  The App then pairs the segments to the original source files.
Once paired, the app creates a blankRush file and we save those in the grade intermediate folder. 
Once the blankrush is made, we  can have everything ready to build the “mirrored source roundtrip files” via the process in runComposition() in main.swift which combines the graded segments with the blankrush which is then used for Online and delivery in PrPro.  
--- 

Let's keep GUI in mind but start with making all the components I think.

The App will likely be MacOS Only to get real ProRes. 
---


Pairing workflow 

Render MM clips from grade project to live intermediate Segments folder as ProRes 4444

Add MM to app Segment group

Add the OCF to app OCF group

Pair them using:
OCF name as the Key 
+ Resolution
+ FPS 
+ SRC TC 
+ Reel Name if available 


SwiftFFmpeg next steps 2025-08-19:
 For Filters: We Need Re-encoding

  To add timecode burn-in filters, we need to:
  1. Decode the compressed ProRes frames to raw pixels
  2. Apply filters (timecode overlay, black frames, etc.)
  3. Re-encode back to ProRes

  This is much more complex than remuxing because we're actually processing the video content.

  Next Steps for Filtering

  We'll need to replace our current remuxing approach with:
  1. Decoder: Decompress input ProRes to raw frames
  2. Filter chain: Apply timecode burn-in, make frames black, etc.
  3. Encoder: Re-compress to ProRes output

  The good news: our current remuxing foundation gives us the correct:
  - Frame rate handling
  - Container setup
  - SwiftFFmpeg patterns

  We can build the decode→filter→encode pipeline on top of this solid base!


  So remuxing was the perfect first step, but for filters we'll need the full decode/encode pipeline.


    What -f lavfi -i "color=black:..." Does

  This is generating completely new black frames instead of using the source video content.

  Breaking it down:
  - -f lavfi = Use FFmpeg's "libavfilter" input format (generates synthetic video)
  - -i "color=black:..." = Generate a synthetic video input with:
    - color=black = Solid black frames
    - size=${FINAL_WIDTH}x${FINAL_HEIGHT} = Same dimensions as source
    - duration=${DURATION} = Same length as source
    - rate=${FPS} = Same frame rate as source

  The Two-Input Strategy

  Looking at the full command:
  ffmpeg \
      -f lavfi -i "color=black:size=${FINAL_WIDTH}x${FINAL_HEIGHT}:duration=${DURATION}:rate=${FPS}" \  # Input 0:
  Generated black video
      -i "$INPUT" \                                                                                        # Input 1:
  Original source file
      -map 0:v -map 1:a? -map_metadata 1 \                                                               # Use video
  from input 0, audio from input 1
      -vf "$DRAWTEXT_FILTER" \                                                                            # Apply
  timecode burn-in to the black video

  So it's:
  1. Generating brand new black frames (not reading source video at all!)
  2. Copying audio tracks from the original file
  3. Copying metadata (including timecode) from original file
  4. Applying timecode burn-in filters to the black video

  Why This Approach?

  This is perfect for "blank rush" creation because:
  - ✅ No decoding of source video needed (fast!)
  - ✅ Black frames are generated synthetically
  - ✅ Same timing as original (duration/framerate matched)
  - ✅ Preserves audio and metadata from source
  - ✅ Only encodes once (black→ProRes with filters)

  This is much more efficient than decode→filter→encode since we skip the decode step entirely!

  So for our SwiftFFmpeg implementation, we could follow this same pattern:
  1. Generate synthetic black frames with SwiftFFmpeg
  2. Apply timecode filters
  3. Encode to ProRes
  4. Copy audio/metadata from source

  Much smarter than trying to decode the source video!
